{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4251e0-07df-4304-b75a-1d0035c6afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from datasets import load_dataset, load_metric, list_metrics\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from scipy.special import softmax\n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d72e9e-a6bb-46e0-9c51-7f22b99e9ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-process tweet data\n",
    "target_labels = ['not_cyberbullying', 'age', 'ethnicity', 'gender', 'religion', 'other_cyberbullying']\n",
    "\n",
    "dataset_df = pd.read_csv(\"data/cyberbullying_tweets/cyberbullying_tweets.csv\")\n",
    "dataset_df.rename(columns = {'cyberbullying_type':'label', 'tweet_text':'text'}, inplace = True)\n",
    "dataset_df['label'] = dataset_df['label'].replace({'not_cyberbullying':0, 'age':1, 'ethnicity':2, 'gender':3, 'religion':4, 'other_cyberbullying':5})\n",
    "dataset_df.to_csv(\"data/cyberbullying_tweets/cyberbullying_tweets.csv\", index=False)\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bd7948d0-d9b5-4ac1-b7c9-86d8bbad0c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration cyberbullying_tweets-6558229128e7786d\n",
      "Reusing dataset csv (/Users/haydenprescott/.cache/huggingface/datasets/csv/cyberbullying_tweets-6558229128e7786d/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e99d63c7ee4798935d79349feec91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set structure: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tweet_text', 'cyberbullying_type'],\n",
      "        num_rows: 47692\n",
      "    })\n",
      "}) \n",
      "Labels: ['age' 'ethnicity' 'gender' 'not_cyberbullying' 'other_cyberbullying'\n",
      " 'religion']\n"
     ]
    }
   ],
   "source": [
    "#load bullying speech dataset and store it\n",
    "dataset = load_dataset(\"data/cyberbullying_tweets\")\n",
    "\n",
    "input_labels = np.unique(np.array(dataset['train']['label']))\n",
    "label_count = len(input_labels)\n",
    "print(\"Data set structure:\", dataset, \"\\nLabels:\", input_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9a177124-405d-4027-92c6-176fee3f2c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the base model to a bert-base-cased\n",
    "model_name = \"bert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "94653e91-a798-465d-87b1-fccf7c085be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /Users/haydenprescott/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /Users/haydenprescott/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /Users/haydenprescott/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /Users/haydenprescott/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /Users/haydenprescott/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the tokenizer from the model and store it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "90f19ae1-8ff5-42bd-b415-3ab83e0c6146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the tokenize function that the tokenizer will be used for\n",
    "def tokenize(samples):\n",
    "    return tokenizer(samples['text'], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "311f7c4d-0ac3-4cd5-a34d-bfab091313a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656c4bd904cf432ababc76add55ad15d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tokenize the data and store it as a new variable, then determine the number of unique labels that will be used to classify the data during training and evaluation\n",
    "dataset_tokens = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0780ec4f-0843-4cdc-9e1a-e8509823e87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval: Dataset({\n",
      "    features: ['tweet_text', 'cyberbullying_type', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 4770\n",
      "}) \n",
      "Train: Dataset({\n",
      "    features: ['tweet_text', 'cyberbullying_type', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 42922\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# randomize the tokenized data, then split it up into a training set and an evaluation set\n",
    "shuffle = dataset_tokens['train'].shuffle(seed=42)\n",
    "\n",
    "train_count = int(shuffle.num_rows * 0.9) \n",
    "\n",
    "dataset_train = shuffle.select(range(0, train_count))\n",
    "dataset_eval = shuffle.select(range(train_count, shuffle.num_rows))\n",
    "print(\"Eval:\", dataset_eval, \"\\nTrain:\", dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "43264197-c63e-4e29-9b1a-16a9c5ab227e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /Users/haydenprescott/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /Users/haydenprescott/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# create an instance of the base model that will be trained\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "54e1952c-8a64-41bd-9373-ee8c2e49b210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# set the characteristics of how the model will be trained (ex: the similarity of the output to the ground truth will be evaluated every epoch)\n",
    "training_args = TrainingArguments(output_dir=\"bullying_model\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3a0f7b40-284c-4dd0-a36a-53b4e6df3338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the accuracy metric\n",
    "acc_metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "da3110df-17b6-473c-b3ba-6610f4bdbf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a fuction that will evaluate the accuracy of the model's output\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return acc_metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "81f0fad9-a196-46c2-b1ca-96a8fcbcb085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the trainer class\n",
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_eval,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44fb22c-8453-407d-92e6-c73ba08568c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: source_data, text, id. If source_data, text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/Users/haydenprescott/opt/anaconda3/envs/hf-transformers/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 9758\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3660\n"
     ]
    }
   ],
   "source": [
    "# train the model!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde48b07-1cde-454c-9b33-7c07b807cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_checkpoint = \"bullying_model/checkpoint-3500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e1770e-dfcf-4e6d-bd6f-049a7fabdc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = AutoModelForSequenceClassification.from_pretrained(ft_model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1785c6bf-cc01-492b-9276-1f31674f4a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"I like cats.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d23240b-58d8-4e18-bbb8-2fe1a20c5c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tokens = tokenizer(sample_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "sample_out = ft_model(**sample_tokens)\n",
    "scores = sample_out[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "\n",
    "labeled_scores = list(tuple(zip([target_labels[idx] for idx in input_labels], scores)))\n",
    "labeled_scores.sort(key=lambda y: y[1], reverse=True)\n",
    "print(labeled_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-transformers",
   "language": "python",
   "name": "hf-transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
